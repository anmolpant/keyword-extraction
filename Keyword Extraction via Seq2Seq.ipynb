{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":27,"outputs":[{"output_type":"stream","text":"/kaggle/input/keyword-data.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport torch\nimport random\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.autograd import variable\nimport torch.nn.functional as F\nimport sys","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_cuda = True\nTRAIN = False","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for arg in sys.argv:\n    if arg =='--train':\n        TRAIN = True\n    elif arg =='--cuda':\n        use_cuda = torch.cuda.is_available()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CUDA :\", use_cuda)\nprint(\"TRAIN: \", TRAIN)","execution_count":31,"outputs":[{"output_type":"stream","text":"CUDA : True\nTRAIN:  False\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Indexing words**: making helper class lang that has word to index and index to word mappings."},{"metadata":{"trusted":true},"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__ (self,name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2 #count of SOS and EOS\n        \n    def add_sentence(self,sentence):\n        for word in sentence.split(' '):\n            self.add_word(word)\n            \n    def add_word(self,word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words +=1\n        else:\n            self.word2count[word] +=1\n            \n\n        ","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading and decoding files from Unicode to ascii**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unicode2ascii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD',s)\n        if unicodedata.category(c) != 'Mn'\n    )","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lowercase, trim and remove non letter characters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_string(s):\n    s = unicode2ascii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the datafile by splitting the file into lines, then splitting the lines into pairs.\nfile mapping titles --> Keywords\nand we want to map from keywords --> titles\nWe use the reverse flag to reverse the pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_langs(lang1,lang2, reverse = False):\n    print(\"Reading lines\")\n    \n    #read the file and split into lines\n    lines = open('../input/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n    \n    #split every line into pairs and normalize\n    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n    \n    #reverse pairs, make Language instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n    \n    return input_lang, output_lang, pairs","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filtering sentences**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 512\n\ndef filter_pair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n\ndef filter_pairs(pairs):\n    return [pair for pair in pairs if filter_pair(pair)]","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"* Read text file\n* Normalization, filter by content\n* Make word lists from sentences in pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n    pairs                          = filter_pairs(pairs)\n\n    for pair in pairs:\n        input_lang.add_sentence(pair[0])\n        output_lang.add_sentence(pair[1])\n\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepare_data('keyword', 'data', False)\n\n#checking\nprint(random.choice(pairs))","execution_count":37,"outputs":[{"output_type":"stream","text":"Reading lines\n['the corsair hydro series keeps your pc cool and silent with maintenance free water cooling', 'cooling water cooling']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## **Building the models**"},{"metadata":{},"cell_type":"markdown","source":"### The Encoder - An RNN that outputs the value for every word from the input sequence. For every word it outputs a vector and a hidden state and uses the hidden state for the next input word."},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__ (self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        \n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1,1,-1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        \n        return output, hidden\n    \n    def init_hidden(self):\n        result = Variable(torch.zeros(1,1,self.hidden_size))\n        \n        if use_cuda:\n            return result.cuda()\n        else:\n            return result\n        ","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Decoder - Output conditioned on the previous outputs and some x, where x consists of the current hidden state (that itself takes into account the previous outputs) and the attention \"context\""},{"metadata":{},"cell_type":"markdown","source":"To summarize, our decoder consists of 4 main parts:\n* An embedding layer - turning the input into a vector.\n* A layer calculating the attention energy per encoder output.\n* ek RNN layer\n* ek output layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__ (self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        \n        #defining parameters\n        self.hidden_size = hidden_size\n        \n        #define layers\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.output = nn.LogSoftmax(dim=1)\n        \n        def forward(self, input, hidden):\n            # we will only be running forward for a single decoder time step, but will use all encoder outputs.\n            \n            output = self.embedding(input).view = (1,1,-1) #S=1\n            output = F.relu(output)\n            output,hidden = self.gru(output,hidden)\n            output = self.softmax(self.out(output[0]))\n            return output, hidden\n        \n        def init_hidden(self):\n            result = Variable(torch.zeros(1,1,self.hidden_size))\n            \n            if use_cuda:\n                return result.cuda()\n            else:\n                return result","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Attention Decoder (neural machine translation to calculate attention context)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        \n        #Define Parameters\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        \n        #define layers\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n        \n        def forward(self, input, hidden, encoder_outputs):\n            #running forward for a single decoder time step but we will use all encoder outputs\n            \n            #get the embedding of the current input word(last input word)\n            embedded = self.embedding(input).view(1,1,-1) # S = 1 X B X N \n            embedded = self.dropout(embedded)\n            \n            #calculate attn weights and apply to encoder outputs\n            attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]),1)), dim=1)\n            #to incorporate context\n            attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n            \n            #final output layer\n            \n            output = torch.cat((embedded[0], attn_applied[0]),1)\n            output = self.attn_combine(output).unsqueeze(0)\n            output = F.relu(output)\n            output, hidden = self.gru(output,hidden)\n            output = F.log_softmax(self.out(output[0]), dim=1)\n            \n            return output, hidden, attn_weights\n        \n        def init_hidden(self):\n            result = Variable(torch.zeros(1,1,self.hidden_size))\n            \n            if use_cuda:\n                return result.cuda()\n            else:\n                return result","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def indexes_from_sentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variable_from_sentence(lang, sentence):\n    indexes = indexes_from_sentence(lang,sentence)\n    indexes.append(EOS_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}