{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":30,"outputs":[{"output_type":"stream","text":"/kaggle/input/keyword-data.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport torch\nimport random\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport sys","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_cuda = True\nTRAIN = False","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for arg in sys.argv:\n    if arg =='--train':\n        TRAIN = True\n    elif arg =='--cuda':\n        use_cuda = torch.cuda.is_available()","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CUDA :\", use_cuda)\nprint(\"TRAIN: \", TRAIN)","execution_count":34,"outputs":[{"output_type":"stream","text":"CUDA : True\nTRAIN:  False\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Indexing words**: making helper class lang that has word to index and index to word mappings."},{"metadata":{"trusted":true},"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__ (self,name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2 #count of SOS and EOS\n        \n    def add_sentence(self,sentence):\n        for word in sentence.split(' '):\n            self.add_word(word)\n            \n    def add_word(self,word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words +=1\n        else:\n            self.word2count[word] +=1\n            \n\n        ","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading and decoding files from Unicode to ascii**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unicode2ascii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD',s)\n        if unicodedata.category(c) != 'Mn'\n    )","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lowercase, trim and remove non letter characters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_string(s):\n    s = unicode2ascii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the datafile by splitting the file into lines, then splitting the lines into pairs.\nfile mapping titles --> Keywords\nand we want to map from keywords --> titles\nWe use the reverse flag to reverse the pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_langs(lang1,lang2, reverse = False):\n    print(\"Reading lines\")\n    \n    #read the file and split into lines\n    lines = open('../input/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n    \n    #split every line into pairs and normalize\n    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n    \n    #reverse pairs, make Language instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n    \n    return input_lang, output_lang, pairs","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filtering sentences**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 512\n\ndef filter_pair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n\ndef filter_pairs(pairs):\n    return [pair for pair in pairs if filter_pair(pair)]","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"* Read text file\n* Normalization, filter by content\n* Make word lists from sentences in pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n    pairs                          = filter_pairs(pairs)\n\n    for pair in pairs:\n        input_lang.add_sentence(pair[0])\n        output_lang.add_sentence(pair[1])\n\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepare_data('keyword', 'data', False)\n\n#checking\nprint(random.choice(pairs))","execution_count":40,"outputs":[{"output_type":"stream","text":"Reading lines\n['buy these four knives instead of an expensive set', 'knives']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## **Building the models**"},{"metadata":{},"cell_type":"markdown","source":"### The Encoder - An RNN that outputs the value for every word from the input sequence. For every word it outputs a vector and a hidden state and uses the hidden state for the next input word."},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__ (self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        \n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1,1,-1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        \n        return output, hidden\n    \n    def init_hidden(self):\n        result = Variable(torch.zeros(1,1,self.hidden_size))\n        \n        if use_cuda:\n            return result.cuda()\n        else:\n            return result\n        ","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Decoder - Output conditioned on the previous outputs and some x, where x consists of the current hidden state (that itself takes into account the previous outputs) and the attention \"context\""},{"metadata":{},"cell_type":"markdown","source":"To summarize, our decoder consists of 4 main parts:\n* An embedding layer - turning the input into a vector.\n* A layer calculating the attention energy per encoder output.\n* ek RNN layer\n* ek output layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__ (self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        \n        #defining parameters\n        self.hidden_size = hidden_size\n        \n        #define layers\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.output = nn.LogSoftmax(dim=1)\n        \n        def forward(self, input, hidden):\n            # we will only be running forward for a single decoder time step, but will use all encoder outputs.\n            \n            output = self.embedding(input).view = (1,1,-1) #S=1\n            output = F.relu(output)\n            output,hidden = self.gru(output,hidden)\n            output = self.softmax(self.out(output[0]))\n            return output, hidden\n        \n        def init_hidden(self):\n            result = Variable(torch.zeros(1,1,self.hidden_size))\n            \n            if use_cuda:\n                return result.cuda()\n            else:\n                return result","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Attention Decoder (neural machine translation to calculate attention context)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        \n        #Define Parameters\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        \n        #define layers\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n        \n        def forward(self, input, hidden, encoder_outputs):\n            #running forward for a single decoder time step but we will use all encoder outputs\n            \n            #get the embedding of the current input word(last input word)\n            embedded = self.embedding(input).view(1,1,-1) # S = 1 X B X N \n            embedded = self.dropout(embedded)\n            \n            #calculate attn weights and apply to encoder outputs\n            attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]),1)), dim=1)\n            #to incorporate context\n            attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n            \n            #final output layer\n            \n            output = torch.cat((embedded[0], attn_applied[0]),1)\n            output = self.attn_combine(output).unsqueeze(0)\n            output = F.relu(output)\n            output, hidden = self.gru(output,hidden)\n            output = F.log_softmax(self.out(output[0]), dim=1)\n            \n            return output, hidden, attn_weights\n        \n        def init_hidden(self):\n            result = Variable(torch.zeros(1,1,self.hidden_size))\n            \n            if use_cuda:\n                return result.cuda()\n            else:\n                return result","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def indexes_from_sentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variable_from_sentence(lang, sentence):\n    indexes = indexes_from_sentence(lang,sentence)\n    indexes.append(EOS_token)\n    \n    result = Variable(torch.LongTensor(indexes).view(-1,1))\n    \n    if use_cuda:\n        return result.cuda()\n    else:\n        return result","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variables_from_pair(pair):\n    input_variable = variable_from_sentence(input_lang, pair[0])\n    target_variable = variable_from_sentence(output_lang,pair[1])\n    \n    return (input_variable, target_variable)","execution_count":46,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training idhar se shuru**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#\"Teacher Forcing\", or maximum likelihood sampling, means using the real target outputs as each next input when training. \n#The alternative is using the decoder's own guess as the next input. Using teacher forcing may cause the network to converge faster.\nteacher_forcing_ratio = 0.5","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will use scheduled sampling to tackle the teacher-forcing problem, which will alternate between using the taget values and the predicted values when training.\n# we will randomy choose to use teacher forcing with training.\n# sometimes use decoder output, sometimes ignore.","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.init_hidden()\n    \n    # Zero gradients of both optimizers\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    \n    # Get size of input and target sentences\n    input_length = input_variable.size()[0]\n    target_length = target_variable.size()[0]\n    \n    # Run words through encoder\n    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n    loss = 0\n    \n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0][0]\n        \n    # prepare input and output variables\n    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n    # use last hidden state from encoder to start decoder.\n    decoder_hidden = encoder_hidden\n    \n    # choose whether to use teacher forcing\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    \n    if use_teacher_forcing:\n       # teacher-forcing: use ground truth target as next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n            decoder_input, decoder_hidden, encoder_outputs\n            )\n            \n            loss += criterion(decoder_output, target_variable[di])\n            decoder_input = target_variable[di]\n    else:\n        # without teacher forcing: use network's own prediction as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n            decoder_input, decoder_hidden, encoder_outputs\n            )\n            \n            #get most likely word index (highest value) from output\n            topv, topi = decoder_output.data.topk(1)\n            ni = topi[0][0]\n            #chosen word is next input\n            decoder_input = Variable(torch.LongTensor([[ni]]))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n            loss += criterion(decoder_output, target_variable[di])\n            \n            if ni == EOS_token:\n                break\n                \n    #Backpropagation\n    loss.backward()\n    \n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    \n    return loss.data[0]/target_length","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Adding helper functions to print time elapsed and estimated time remaining, given the current time and progress."},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport time","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def as_minutes(s):\n    m = math.floor(s / 60)\n    s -=m*60\n    \n    return '%dm %ds' % (m, s)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_since(since, percent):\n    now = time.time()\n    s   = now - since\n    es  = s / (percent)\n    rs  = es - s\n\n    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we can start training","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize models, optimizers and iterations\ndef train_iterations(encoder, decoder, n_iters, print_every = 1000, learning_rate =0.01):\n    start = time.time()\n    print_loss_total = 0\n    \n    ## Initialize optimizers and criterion\n    \n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs    = [variables_from_pair(random.choice(pairs)) for i in range(n_iters)]\n    criterion         = nn.NLLLoss()\n    \n    #beginnn\n    for iter in range(1, n_iters + 1):\n        training_pair   = training_pairs[iter - 1]\n        input_variable  = training_pair[0]\n        target_variable = training_pair[1]\n        \n     #keep track of loss\n        loss = train(\n            input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion\n        )\n\n        print_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg   = print_loss_total / print_every\n            print_loss_total = 0  # Reset every print_every\n\n            print('%s (%d %d%%) %.4f' % (\n                time_since(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg\n            ))\n    ","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    input_variable  = variable_from_sentence(input_lang, sentence)\n    input_length    = input_variable.size()[0]\n    encoder_hidden  = encoder.init_hidden()\n    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n    encoder_outputs = encoder_ouputs.cuda() if use_cuda else encoder_ouputs\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n        encoder_outputs[ei]            = encoder_outputs[ei] + encoder_output[0][0]\n\n    decoder_input      = Variable(torch.LongTensor([[SOS_token]]))\n    decoder_input      = decoder_input.cuda() if use_cuda else decoder_input\n    decoder_hidden     = encoder_hidden\n    decoded_words      = []\n    decoder_attentions = torch.zeros(max_length, max_length)\n\n    for di in range(max_length):\n        decoder_output, decoder_hidden, decoder_attention = decoder(\n            decoder_input, decoder_hidden, encoder_outputs\n        )\n\n        decoder_attentions[di] = decoder_attention.data\n        topv, topi             = decoder_output.data.topk(1)\n        ni                     = topi[0][0]\n\n        if ni == EOS_token:\n            decoded_words.append('<EOS>')\n            break\n        else:\n            decoded_words.append(output_lang.index2word[ni])\n\n        decoder_input = Variable(torch.LongTensor([[ni]]))\n        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    return decoded_words, decoder_attentions[:di + 1]","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_size = 256","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = True","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN is True:\n    print(\"TRAINING...\")\n\n    encoder1 = EncoderRNN(input_lang.n_words, hidden_size)\n    attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1)\n\n    if use_cuda:\n        encoder1 = encoder1.cuda()\n        attn_decoder1 = attn_decoder1.cuda()\n\n    train_iterations(encoder1, attn_decoder1, 75000, print_every=5000)\n\n    torch.save(encoder1, 'encoder.pt')\n    torch.save(attn_decoder1, 'decoder.pt')\nelse:\n    print(\"LOADING...\")\n\n    encoder1      = torch.load('encoder.pt')\n    attn_decoder1 = torch.load('decoder.pt')","execution_count":60,"outputs":[{"output_type":"stream","text":"TRAINING...\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'Variable' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-6718d4b71b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoder.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-df6d35382299>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(encoder, decoder, n_iters, print_every, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtraining_pairs\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvariables_from_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcriterion\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-df6d35382299>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtraining_pairs\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvariables_from_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcriterion\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-d33d18f354cd>\u001b[0m in \u001b[0;36mvariables_from_pair\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvariables_from_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-45-406a25611c30>\u001b[0m in \u001b[0;36mvariable_from_sentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def output_evaluation(input_sentence):\n    output_words, attentions = evaluate(\n        encoder1, attn_decoder1, input_sentence\n    )\n\n    print(\"input  = \", input_sentence)\n    print(\"output = \", ' '.join(output_words))\n\nwhile(True):\n    try:\n        inp = raw_input(\">\")\n        output_evaluation(inp)\n    except KeyError:\n        pass","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}