Normative System Games
Thomas
◦
Agotnes
Dept of Computer Engineering
Bergen University College
PB. 2030, N-5020 Bergen
Norway
tag@hib.no
Wiebe van der Hoek
Dept of Computer Science
University of Liverpool
Liverpool L69 7ZF
UK
wiebe@csc.liv.ac.uk
Michael Wooldridge
Dept of Computer Science
University of Liverpool
Liverpool L69 7ZF
UK
mjw@csc.liv.ac.uk
ABSTRACT
We develop a model of normative systems in which agents are 
assumed to have multiple goals of increasing priority, and 
investigate the computational complexity and game theoretic properties of
this model. In the underlying model of normative systems, we use
Kripke structures to represent the possible transitions of a 
multiagent system. A normative system is then simply a subset of the
Kripke structure, which contains the arcs that are forbidden by the
normative system. We specify an agent"s goals as a hierarchy of
formulae of Computation Tree Logic (CTL), a widely used logic
for representing the properties of Kripke structures: the intuition is
that goals further up the hierarchy are preferred by the agent over
those that appear further down the hierarchy. Using this scheme,
we define a model of ordinal utility, which in turn allows us to
interpret our Kripke-based normative systems as games, in which
agents must determine whether to comply with the normative 
system or not. We then characterise the computational complexity of
a number of decision problems associated with these Kripke-based
normative system games; for example, we show that the 
complexity of checking whether there exists a normative system which has
the property of being a Nash implementation is NP-complete.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems;
I.2.4 [Knowledge representation formalisms and methods]
General Terms
Theory
1. INTRODUCTION
Normative systems, or social laws, have proved to be an attractive
approach to coordination in multi-agent systems [13, 14, 10, 15, 1].
Although the various approaches to normative systems proposed in
the literature differ on technical details, they all share the same 
basic intuition that a normative system is a set of constraints on the
behaviour of agents in the system; by imposing these constraints,
it is hoped that some desirable objective will emerge. The idea of
using social laws to coordinate multi-agent systems was proposed
by Shoham and Tennenholtz [13, 14]; their approach was extended
by van der Hoek et al. to include the idea of specifying a desirable
global objective for a social law as a logical formula, with the idea
being that the normative system would be regarded as successful
if, after implementing it (i.e., after eliminating all forbidden 
actions), the objective formula was guaranteed to be satisfied in the
system [15]. However, this model did not take into account the
preferences of individual agents, and hence neglected to account
for possible strategic behaviour by agents when deciding whether
to comply with the normative system or not. This model of 
normative systems was further extended by attributing to each agent
a single goal in [16]. However, this model was still too 
impoverished to capture the kinds of decision making that take place when
an agent decides whether or not to comply with a social law. In
reality, strategic considerations come into play: an agent takes into
account not just whether the normative system would be beneficial
for itself, but also whether other agents will rationally choose to
participate.
In this paper, we develop a model of normative systems in which
agents are assumed to have multiple goals, of increasing priority.
We specify an agent"s goals as a hierarchy of formulae of 
Computation Tree Logic (CTL), a widely used logic for representing the
properties of Kripke structures [8]: the intuition is that goals further
up the hierarchy are preferred by the agent over those that appear
further down the hierarchy. Using this scheme, we define a model
of ordinal utility, which in turn allows us to interpret our 
Kripkebased normative systems as games, in which agents must determine
whether to comply with the normative system or not. We thus 
provide a very natural bridge between logical structures and languages
and the techniques and concepts of game theory, which have proved
to be very powerful for analysing social contract-style scenarios
such as normative systems [3, 4]. We then characterise the 
computational complexity of a number of decision problems associated
with these Kripke-based normative system games; for example, we
show that the complexity of checking whether there exists a 
normative system which has the property of being a Nash implementation
is NP-complete.
2. KRIPKE STRUCTURES AND CTL
We use Kripke structures as our basic semantic model for 
multiagent systems [8]. A Kripke structure is essentially a directed
graph, with the vertex set S corresponding to possible states of the
system being modelled, and the relation R ⊆ S × S capturing the
881
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
possible transitions of the system; intuitively, these transitions are
caused by agents in the system performing actions, although we do
not include such actions in our semantic model (see, e.g., [13, 2,
15] for related models which include actions as first class citizens).
We let S0
denote the set of possible initial states of the system.
Our model is intended to correspond to the well-known interleaved
concurrency model from the reactive systems literature: thus an
arc corresponds to the execution of an atomic action by one of the
processes in the system, which we call agents.
It is important to note that, in contrast to such models as [2, 15],
we are therefore here not modelling synchronous action. This 
assumption is not in fact essential for our analysis, but it greatly 
simplifies the presentation. However, we find it convenient to include
within our model the agents that cause transitions. We therefore
assume a set A of agents, and we label each transition in R with
the agent that causes the transition via a function α : R → A. 
Finally, we use a vocabulary Φ = {p, q, . . .} of Boolean variables
to express the properties of individual states S: we use a function
V : S → 2Φ
to label each state with the Boolean variables true (or
satisfied) in that state.
Collecting these components together, an agent-labelled Kripke
structure (over Φ) is a 6-tuple:
K = S, S0
, R, A, α, V , where:
• S is a finite, non-empty set of states,
• S0
⊆ S (S0
= ∅) is the set of initial states;
• R ⊆ S × S is a total binary relation on S, which we refer to
as the transition relation1
;
• A = {1, . . . , n} is a set of agents;
• α : R → A labels each transition in R with an agent; and
• V : S → 2Φ
labels each state with the set of propositional
variables true in that state.
In the interests of brevity, we shall hereafter refer to an 
agentlabelled Kripke structure simply as a Kripke structure. A path
over a transition relation R is an infinite sequence of states π =
s0, s1, . . . which must satisfy the property that ∀u ∈ N: (su , su+1) ∈
R. If u ∈ N, then we denote by π[u] the component indexed by
u in π (thus π[0] denotes the first element, π[1] the second, and so
on). A path π such that π[0] = s is an s-path. Let ΠR(s) denote
the set of s-paths over R; since it will usually be clear from 
context, we often omit reference to R, and simply write Π(s). We will
sometimes refer to and think of an s-path as a possible 
computation, or system evolution, from s.
EXAMPLE 1. Our running example is of a system with a single
non-sharable resource, which is desired by two agents. Consider
the Kripke structure depicted in Figure 1. We have two states, s and
t, and two corresponding Boolean variables p1 and p2, which are
1
In the branching time temporal logic literature, a relation R ⊆
S × S is said to be total iff ∀s ∃s : (s, s ) ∈ R. Note that
the term total relation is sometimes used to refer to relations
R ⊆ S × S such that for every pair of elements s, s ∈ S we
have either (s, s ) ∈ R or (s , s) ∈ R; we are not using the term
in this way here. It is also worth noting that for some domains,
other constraints may be more appropriate than simple totality. For
example, one might consider the agent totality requirement, that in
every state, every agent has at least one possible transition 
available: ∀s∀i ∈ A∃s : (s, s ) ∈ R and α(s, s ) = i.
2p
t
p
2
2
1
s
1
1
Figure 1: The resource control running example.
mutually exclusive. Think of pi as meaning agent i has currently
control over the resource. Each agent has two possible actions,
when in possession of the resource: either give it away, or keep it.
Obviously there are infinitely many different s-paths and t-paths.
Let us say that our set of initial states S0
equals {s, t}, i.e., we
don"t make any assumptions about who initially has control over
the resource.
2.1 CTL
We now define Computation Tree Logic (CTL), a branching time
temporal logic intended for representing the properties of Kripke
structures [8]. Note that since CTL is well known and widely 
documented in the literature, our presentation, though complete, will be
somewhat terse. We will use CTL to express agents" goals.
The syntax of CTL is defined by the following grammar:
ϕ ::= | p | ¬ϕ | ϕ ∨ ϕ | E fϕ | E(ϕ U ϕ) | A fϕ | A(ϕ U ϕ)
where p ∈ Φ. We denote the set of CTL formula over Φ by LΦ;
since Φ is understood, we usually omit reference to it.
The semantics of CTL are given with respect to the satisfaction
relation |=, which holds between pairs of the form K, s, (where
K is a Kripke structure and s is a state in K), and formulae of the
language. The satisfaction relation is defined as follows:
K, s |= ;
K, s |= p iff p ∈ V (s) (where p ∈ Φ);
K, s |= ¬ϕ iff not K, s |= ϕ;
K, s |= ϕ ∨ ψ iff K, s |= ϕ or K, s |= ψ;
K, s |= A fϕ iff ∀π ∈ Π(s) : K, π[1] |= ϕ;
K, s |= E fϕ iff ∃π ∈ Π(s) : K, π[1] |= ϕ;
K, s |= A(ϕ U ψ) iff ∀π ∈ Π(s), ∃u ∈ N, s.t. K, π[u] |= ψ
and ∀v, (0 ≤ v < u) : K, π[v] |= ϕ
K, s |= E(ϕ U ψ) iff ∃π ∈ Π(s), ∃u ∈ N, s.t. K, π[u] |= ψ
and ∀v, (0 ≤ v < u) : K, π[v] |= ϕ
The remaining classical logic connectives (∧, →, ↔) are
assumed to be defined as abbreviations in terms of ¬, ∨, in the
conventional manner. The remaining CTL temporal operators are
defined:
A♦ϕ ≡ A( U ϕ) E♦ϕ ≡ E( U ϕ)
A ϕ ≡ ¬E♦¬ϕ E ϕ ≡ ¬A♦¬ϕ
We say ϕ is satisfiable if K, s |= ϕ for some Kripke structure K
and state s in K; ϕ is valid if K, s |= ϕ for all Kripke structures
K and states s in K. The problem of checking whether K, s |= ϕ
for given K, s, ϕ (model checking) can be done in deterministic
polynomial time, while checking whether a given ϕ is satisfiable or
whether ϕ is valid is EXPTIME-complete [8]. We write K |= ϕ if
K, s0 |= ϕ for all s0 ∈ S0
, and |= ϕ if K |= ϕ for all K.
882 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
3. NORMATIVE SYSTEMS
For our purposes, a normative system is simply a set of constraints
on the behaviour of agents in a system [1]. More precisely, a 
normative system defines, for every possible system transition, whether
or not that transition is considered to be legal or not. Different
normative systems may differ on whether or not a transition is
legal. Formally, a normative system η (w.r.t. a Kripke structure
K = S, S0
, R, A, α, V ) is simply a subset of R, such that R \ η
is a total relation. The requirement that R\η is total is a 
reasonableness constraint: it prevents normative systems which lead to states
with no successor. Let N (R) = {η : (η ⊆ R) & (R \ η is total)}
be the set of normative systems over R. The intended 
interpretation of a normative system η is that (s, s ) ∈ η means transition
(s, s ) is forbidden in the context of η; hence R \ η denotes the
legal transitions of η. Since it is assumed η is reasonable, we are
guaranteed that a legal outward transition exists for every state. We
denote the empty normative system by η∅, so η∅ = ∅. Note that
the empty normative system η∅ is reasonable with respect to any
transition relation R.
The effect of implementing a normative system on a Kripke 
structure is to eliminate from it all transitions that are forbidden 
according to this normative system (see [15, 1]). If K is a Kripke 
structure, and η is a normative system over K, then K † η denotes the
Kripke structure obtained from K by deleting transitions forbidden
in η. Formally, if K = S, S0
, R, A, α, V , and η ∈ N (R), then
let K†η = K be the Kripke structure K = S , S0
, R , A , α , V
where:
• S = S , S0
= S0
, A = A , and V = V ;
• R = R \ η; and
• α is the restriction of α to R :
α (s, s ) =
j
α(s, s ) if (s, s ) ∈ R
undefined otherwise.
Notice that for all K, we have K † η∅ = K.
EXAMPLE 1. (continued) When thinking in terms of fairness, it
seems natural to consider normative systems η that contain (s, s)
or (t, t). A normative system with (s, t) would not be fair, in the
sense that A♦A ¬p1 ∨ A♦A ¬p2 holds: in all paths, from
some moment on, one agent will have control forever. Let us, for
later reference, fix η1 = {(s, s)}, η2 = {(t, t)}, and η3 = {(s, s),
(t, t)}.
Later, we will address the issue of whether or not agents should
rationally choose to comply with a particular normative system. In
this context, it is useful to define operators on normative systems
which correspond to groups of agents defecting from the 
normative system. Formally, let K = S, S0
,R, A,α, V be a Kripke
structure, let C ⊆ A be a set of agents over K, and let η be a
normative system over K. Then:
• η C denotes the normative system that is the same as η
except that it only contains the arcs of η that correspond to
the actions of agents in C. We call η C the restriction of η
to C, and it is defined as:
η C = {(s, s ) : (s, s ) ∈ η & α(s, s ) ∈ C}.
Thus K † (η C) is the Kripke structure that results if only
the agents in C choose to comply with the normative system.
• η C denotes the normative system that is the same as η 
except that it only contains the arcs of η that do not correspond
to actions of agents in C. We call η C the exclusion of C
from η, and it is defined as:
η C = {(s, s ) : (s, s ) ∈ η & α(s, s ) ∈ C}.
Thus K † (η C) is the Kripke structure that results if only
the agents in C choose not to comply with the normative
system (i.e., the only ones who comply are those in A \ C).
Note that we have η C = η (A\C) and η C = η (A\C).
EXAMPLE 1. (Continued) We have η1 {1} = η1 = {(s, s)},
while η1 {1} = η∅ = η1 {2}. Similarly, we have η3 {1} =
{(s, s)} and η3 {1} = {(t, t)}.
4. GOALS AND UTILITIES
Next, we want to be able to capture the goals that agents have, as
these will drive an agent"s strategic considerations - particularly, as
we will see, considerations about whether or not to comply with a
normative system. We will model an agent"s goals as a prioritised
list of CTL formulae, representing increasingly desired properties
that the agent wishes to hold. The intended interpretation of such a
goal hierarchy γi for agent i ∈ A is that the further up the 
hierarchy a goal is, the more it is desired by i. Note that we assume
that if an agent can achieve a goal at a particular level in its goal
hierarchy, then it is unconcerned about goals lower down the 
hierarchy. Formally, a goal hierarchy, γ, (over a Kripke structure K)
is a finite, non-empty sequence of CTL formulae
γ = (ϕ0, ϕ1, . . . , ϕk )
in which, by convention, ϕ0 = . We use a natural number 
indexing notation to extract the elements of a goal hierarchy, so if
γ = (ϕ0, ϕ1, . . . , ϕk ) then γ[0] = ϕ0, γ[1] = ϕ1, and so on. We
denote the largest index of any element in γ by |γ|.
A particular Kripke structure K is said to satisfy a goal at 
index x in goal hierarchy γ if K |= γ[x], i.e., if γ[x] is satisfied in all
initial states S0
of K. An obvious potential property of goal 
hierarchies is monotonicity: where goals at higher levels in the hierarchy
logically imply those at lower levels in the hierarchy. Formally, a
goal hierarchy γ is monotonic if for all x ∈ {1, . . . , |γ|} ⊆ N, we
have |= γ[x] → γ[x − 1]. The simplest type of monotonic goal
hierarchy is where γ[x + 1] = γ[x] ∧ ψx+1 for some ψx+1, so at
each successive level of the hierarchy, we add new constraints to
the goal of the previous level. Although this is a natural property
of many goal hierarchies, it is not a property we demand of all goal
hierarchies.
EXAMPLE 1. (continued) Suppose the agents have similar, but
opposing goals: each agent i wants to keep the source as often and
long as possible for himself. Define each agent"s goal hierarchy as:
γi = ( ϕi
0 = , ϕi
1 = E♦pi ,
ϕi
2 = E E♦pi , ϕi
3 = E♦E pi ,
ϕi
4 = A E♦pi , ϕi
5 = E♦A pi
ϕi
6 = A A♦pi , ϕi
7 = A (A♦pi ∧ E pi ),
ϕi
8 = A pi )
The most desired goal of agent i is to, in every computation, 
always have the resource, pi (this is expressed in ϕi
8). Thanks to our
reasonableness constraint, this goal implies ϕi
7 which says that, no
matter how the computation paths evolve, it will always be that all
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 883
continuations will hit a point in which pi , and, moreover, there is a
continuation in which pi always holds. Goal ϕi
6 is a fairness 
constraint implied by it. Note that A♦pi says that every computation
eventually reaches a pi state. This may mean that after pi has 
happened, it will never happen again. ϕi
6 circumvents this: it says that,
no matter where you are, there should be a future pi state. The goal
ϕi
5 is like the strong goal ϕi
8 but it accepts that this is only achieved
in some computation, eventually. ϕi
4 requires that in every path,
there is always a continuation that eventually gives pi . Goal ϕi
3
says that pi should be true on some branch, from some moment on.
It implies ϕi
2 which expresses that there is a computation such that
everywhere during it, it is possible to choose a continuation that
eventually satisfies pi . This implies ϕi
1, which says that pi should
at least not be impossible. If we even drop that demand, we have
the trivial goal ϕi
0.
We remark that it may seem more natural to express a fairness
constraint ϕi
6 as A ♦pi . However, this is not a proper CTL 
formula. It is in fact a formula in CTL
∗
[9], and in this logic, the two
expressions would be equivalent. However, our basic complexity
results in the next sections would not hold for the richer language
CTL
∗2
, and the price to pay for this is that we have to formulate
our desired goals in a somewhat more cumbersome manner than
we might ideally like. Of course, our basic framework does not
demand that goals are expressed in CTL; they could equally well
be expressed in CTL
∗
or indeed ATL [2] (as in [15]). We 
comment on the implications of alternative goal representations at the
conclusion of the next section.
A multi-agent system collects together a Kripke structure 
(representing the basic properties of a system under consideration: its
state space, and the possible state transitions that may occur in it),
together with a goal hierarchy, one for each agent, representing the
aspirations of the agents in the system. Formally, a multi-agent
system, M , is an (n + 1)-tuple:
M = K, γ1, . . . , γn
where K is a Kripke structure, and for each agent i in K, γi is a
goal hierarchy over K.
4.1 The Utility of Normative Systems
We can now define the utility of a Kripke structure for an agent.
The idea is that the utility of a Kripke structure is the highest index
of any goal that is guaranteed for that agent in the Kripke structure.
We make this precise in the function ui (·):
ui (K) = max{j : 0 ≤ j ≤ |γi | & K |= γi [j ]}
Note that using these definitions of goals and utility, it never
makes sense to have a goal ϕ at index n if there is a logically
weaker goal ψ at index n + k in the hierarchy: by definition of
utility, it could never be n for any structure K.
EXAMPLE 1. (continued) Let M = K, γ1, γ2 be the 
multiagent system of Figure 1, with γ1 and γ2 as defined earlier in this
example. Recall that we have defined S0
as {s, t}. Then, u1(K) =
u2(K) = 4: goal ϕ4 is true in S0
, but ϕ5 is not. To see that
ϕ2
4 = A E♦p2 is true in s for instance: note that on ever path it
is always the case that there is a transition to t, in which p2 is true.
Notice that since for any goal hierarchy γi we have γ[0] = ,
then for all Kripke structures, ui (K) is well defined, with ui (K) ≥
2
CTL
∗
model checking is PSPACE-complete, and hence much
worse (under standard complexity theoretic assumptions) than
model checking CTL [8].
η δ1(K, η) δ2(K, η)
η∅ 0 0
η1 0 3
η2 3 0
η3 2 2
C D
C (2, 2) (0, 3)
D (3, 0) (0, 0)
Figure 2: Benefits of implementing a normative system η (left)
and pay-offs for the game ΣM .
0. Note that this is an ordinal utility measure: it tells us, for any
given agent, the relative utility of different Kripke structures, but
utility values are not on some standard system-wide scale. The fact
that ui (K1) > ui (K2) certainly means that i strictly prefers K1
over K2, but the fact that ui (K) > uj (K) does not mean that i
values K more highly than j . Thus, it does not make sense to 
compare utility values between agents, and so for example, some system
wide measures of utility, (notably those measures that aggregate 
individual utilities, such as social welfare), do not make sense when
applied in this setting. However, as we shall see shortly, other 
measures - such as Pareto efficiency - can be usefully applied.
There are other representations for goals, which would allow us
to define cardinal utilities. The simplest would be to specify goals γ
for an agent as a finite, non-empty, one-to-one relation: γ ⊆ L×R.
We assume that the x values in pairs (ϕ, x) ∈ γ are specified so
that x for agent i means the same as x for agent j , and so we have
cardinal utility. We then define the utility for i of a Kripke structure
K asui (K) = max{x : (ϕ, x) ∈ γi & K |= ϕ}. The results of
this paper in fact hold irrespective of which of these representations
we actually choose; we fix upon the goal hierarchy approach in the
interests of simplicity.
Our next step is to show how, in much the same way, we can lift
the utility function from Kripke structures to normative systems.
Suppose we are given a multi-agent system M = K, γ1, . . . , γn
and an associated normative system η over K. Let for agent i,
δi (K, K ) be the difference in his utility when moving from K to
K : δi (K, K ) = ui (K )− ui (K). Then the utility of η to agent i
wrt K is δi (K, K † η). We will sometimes abuse notation and just
write δi (K, η) for this, and refer to it as the benefit for agent i of
implementing η in K. Note that this benefit can be negative.
Summarising, the utility of a normative system to an agent is the
difference between the utility of the Kripke structure in which the
normative system was implemented and the original Kripke 
structure. If this value is greater than 0, then the agent would be better
off if the normative system were imposed, while if it is less than
0 then the agent would be worse off if η were imposed than in the
original system. We say η is individually rational for i wrt K if
δi (K, η) > 0, and individually rational simpliciter if η is 
individually rational for every agent.
A social system now is a pair
Σ = M , η
where M is a multi-agent system, and η is a normative system over
M .
EXAMPLE 1. The table at the left hand in Figure 2 displays the
utilities δi (K, η) of implementing η in the Kripke structure of our
running example, for the normative systems η = η∅, η1, η2 and η3,
introduced before. Recall that u1(K) = u2(K) = 4.
4.2 Universal and Existential Goals
884 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Keeping in mind that a norm η restricts the possible transitions
of the model under consideration, we make the following 
observation, borrowing from [15]. Some classes of goals are monotonic
or anti-monotonic with respect to adding additional constraints to
a system. Let us therefore define two fragments of the language
of CTL: the universal language Lu
with typical element μ, and the
existential fragment Le
with typical element ε.
μ ::= | p | ¬p | μ ∨ μ | A fμ | A μ | A(μ U μ)
ε ::= | p | ¬p | ε ∨ ε | E fε | E♦ε | E(ε U ε)
Let us say, for two Kripke structures K1 = S, S0
, R1, A, α, V
and K2 = S, S0
, R2, A, α, V that K1 is a subsystem of K2 and
K2 is a supersystem of K1, written K1 K2 iff R1 ⊆ R2. Note
that typically K † η K. Then we have (cf. [15]).
THEOREM 1. Suppose K1 K2, and s ∈ S. Then
∀ε ∈ Le
: K1, s |= ε ⇒ K2, s |= ε
∀μ ∈ Lu
: K2, s |= μ ⇒ K1, s |= μ
This has the following effect on imposing a new norm:
COROLLARY 1. Let K be a structure, and η a normative 
system. Let γi denote a goal hierarchy for agent i.
1. Suppose agent i"s utility ui (K) is n, and γi [n] ∈ Lu
, (i.e.,
γi [n] is a universal formula). Then, for any normative system
η, δi (K, η) ≥ 0.
2. Suppose agent i"s utility ui (K † η) is n, and γi [n] is an
existential formula ε. Then, δi (K † η, K) ≥ 0.
Corollary 1"s first item says that an agent whose current 
maximal goal in a system is a universal formula, need never fear the
imposition of a new norm η. The reason is that his current goal will
at least remain true (in fact a goal higher up in the hierarchy may
become true). It follows from this that an agent with only universal
goals can only gain from the imposition of normative systems η.
The opposite is true for existential goals, according to the second
item of the corollary: it can never be bad for an agent to undo a
norm η. Hence, an agent with only existential goals might well fear
any norm η.
However, these observations implicitly assume that all agents in
the system will comply with the norm. Whether they will in fact do
so, of course, is a strategic decision: it partly depends on what the
agent thinks that other agents will do. This motivates us to consider
normative system games.
5. NORMATIVE SYSTEM GAMES
We now have a principled way of talking about the utility of 
normative systems for agents, and so we can start to apply the technical
apparatus of game theory to analyse them.
Suppose we have a multi-agent system M = K, γ1, . . . , γn
and a normative system η over K. It is proposed to the agents
in M that η should be imposed on K, (typically to achieve some
coordination objective). Our agent - let"s say agent i - is then faced
with a choice: should it comply with the strictures of the normative
system, or not? Note that this reasoning takes place before the agent
is in the system - it is a design time consideration.
We can understand the reasoning here as a game, as follows. A
game in strategic normal form (cf. [11, p.11]) is a structure:
G = AG, S1, . . . , Sn , U1, . . . , Un where:
• AG = {1, . . . , n} is a set of agents - the players of the game;
• Si is the set of strategies for each agent i ∈ AG (a strategy
for an agent i is nothing else than a choice between 
alternative actions); and
• Ui : (S1 × · · · × Sn ) → R is the utility function for agent
i ∈ AG, which assigns a utility to every combination of
strategy choices for the agents.
Now, suppose we are given a social system Σ = M , η where
M = K, γ1, . . . , γn . Then we can associate a game - the 
normative system game - GΣ with Σ, as follows. The agents AG in GΣ
are as in Σ. Each agent i has just two strategies available to it:
• C - comply (cooperate) with the normative system; and
• D - do not comply with (defect from) the normative system.
If S is a tuple of strategies, one for each agent, and x ∈ {C, D},
then we denote by AGx
S the subset of agents that play strategy x in
S. Hence, for a social system Σ = M , η , the normative system
η AGC
S only implements the restrictions for those agents that
choose to cooperate in GΣ. Note that this is the same as η AGD
S :
the normative system that excludes all the restrictions of agents that
play D in GΣ. We then define the utility functions Ui for each
i ∈ AG as:
Ui (S) = δi (K, η AGC
S ).
So, for example, if SD is a collection of strategies in which every
agent defects (i.e., does not comply with the norm), then
Ui (SD ) = δi (K, (η AGD
SD
)) = ui (K † η∅) − ui (K) = 0.
In the same way, if SC is a collection of strategies in which every
agent cooperates (i.e., complies with the norm), then
Ui (SC ) = δi (K, (η AGD
SC
)) = ui (K † (η ∅)) = ui (K † η).
We can now start to investigate some properties of normative
system games.
EXAMPLE 1. (continued) For our example system, we have 
displayed the different U values for our multi agent system with the
norm η3, i.e., {(s, s), (t, t)} as the second table of Figure 2. For
instance, the pair (0, 3) in the matrix under the entry S = C, D
is obtained as follows. U1( C, D ) = δ1(K, η3 AGC
C,D ) =
u1(K † η3 AGC
C,D ) − u1(K). The first term of this is the
utility of 1 in the system K where we implement η3 for the 
cooperating agent, i.e., 1, only. This means that the transitions are
R \ {(s, s)}. In this system, still ϕ1
4 = A E♦p1 is the highest
goal for agent 1. This is the same utility for 1 as in K, and hence,
δ1(K, η3 AGC
C,D ) = 0. Agent 2 of course benefits if agent 1
complies with η3 while 2 does not. His utility would be 3, since
η3 AGC
C,D is in fact η1.
5.1 Individually Rational Normative Systems
A normative system is individually rational if every agent would
fare better if the normative system were imposed than otherwise.
This is a necessary, although not sufficient condition on a norm to
expect that everybody respects it. Note that η3 of our example is
individually rational for both 1 and 2, although this is not a stable
situation: given that the other plays C, i is better of by playing
D. We can easily characterise individually rationality with respect
to the corresponding game in strategic form, as follows. Let Σ =
M , η be a social system. Then the following are equivalent:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 885
f(xk)
...
s0
s1
s2
s3
s4
s(2k−1)
s2k
t(x1)
f(x1)
t(x2)
f(x2)
t(xk)
Figure 3: The Kripke structure produced in the reduction of
Theorem 2; all transitions are associated with agent 1, the only
initial state is s0.
1. η is individually rational in M ;
2. ∀i ∈ AG, Ui (SC ) > Ui (SD ) in the game GΣ.
The decision problem associated with individually rational 
normative systems is as follows:
INDIVIDUALLY RATIONAL NORMATIVE SYSTEM (IRNS):
Given: Multi-agent system M .
Question: Does there exist an individually rational 
normative system for M ?
THEOREM 2. IRNS is NP-complete, even in one-agent systems.
PROOF. For membership of NP, guess a normative system η,
and verify that it is individually rational. Since η ⊆ R, we will be
able to guess it in nondeterministic polynomial time. To verify that
it is individually rational, we check that for all i, we have ui (K †
η) > ui (K); computing K † η is just set subtraction, so can be
done in polynomial time, while determining the value of ui (K) for
any K can be done with a polynomial number of model checking
calls, each of which requires only time polynomial in the K and γ.
Hence verifying that ui (K † η) > ui (K) requires only polynomial
time.
For NP-hardness, we reduce SAT [12, p.77]. Given a SAT instance
ϕ over Boolean variables x1, . . . , xk , we produce an instance of
IRNS as follows. First, we define a single agent A = {1}. For each
Boolean variable xi in the SAT instance, we create two Boolean
variables t(xi ) and f (xi ) in the IRNS instance. We then create a
Kripke structure Kϕ with 2k + 1 states, as shown in Figure 3: arcs
in this graph correspond to transitions in Kϕ. Let ϕ∗
be the result
of systematically substituting for every Boolean variable xi in ϕ
the CTL expression (E ft(xi )). Next, consider the following 
formulae:
k^
i=1
E f(t(xi ) ∨ f (xi )) (1)
k^
i=1
¬((E ft(xi )) ∧ (E ff (xi ))) (2)
We then define the goal hierarchy for all agent 1 as follows:
γ1[0] =
γ1[1] = (1) ∧ (2) ∧ ϕ∗
We claim there is an individually rational normative system for the
instance so constructed iff ϕ is satisfiable. First, notice that any
individually rational normative system must force γ1[1] to be true,
since in the original system, we do not have γ1[1].
For the ⇒ direction, if there is an individually rational normative
system η, then we construct a satisfying assignment for ϕ by 
considering the arcs that are forbidden by η: formula (1) ensures that
we must forbid an arc to either a t(xi ) or a f (xi ) state for all 
variables xi , but (2) ensures that we cannot forbid arcs to both. So, if
we forbid an arc to a t(xi ) state then in the corresponding valuation
for ϕ we make xi false, while if we forbid an arc to a f (xi ) state
then we make xi true. The fact that ϕ∗
is part of the goal ensures
that the normative system is indeed a valuation for ϕ.
For ⇐, note that for any satisfying valuation for ϕ we can 
construct an individually rational normative system η, as follows: if
the valuation makes xi true, we forbid the arc to the f (xi ) state,
while if the valuation makes xi false, we forbid the arc to the t(xi )
state. The resulting normative system ensures γ1[1], and is thus
individually rational.
Notice that the Kripke structure constructed in the reduction 
contains just a single agent, and so the Theorem is proven.
5.2 Pareto Efficient Normative Systems
Pareto efficiency is a basic measure of how good a particular
outcome is for a group of agents [11, p.7]. Intuitively, an outcome
is Pareto efficient if there is no other outcome that makes every
agent better off. In our framework, suppose we are given a social
system Σ = M , η , and asked whether η is Pareto efficient. This
amounts to asking whether or not there is some other normative
system η such that every agent would be better off under η than
with η. If η makes every agent better off than η, then we say η
Pareto dominates η. The decision problem is as follows:
PARETO EFFICIENT NORMATIVE SYSTEM (PENS):
Given: Multi-agent system M and normative system η
over M .
Question: Is η Pareto efficient for M ?
THEOREM 3. PENS is co-NP-complete, even for one-agent 
systems.
PROOF. Let M and η be as in the Theorem. We show that the
complement problem to PENS, which we refer to as PARETO 
DOMINATED, is NP-complete. In this problem, we are given M and η,
and we are asked whether η is Pareto dominated, i.e., whether or not
there exists some η over M such that η makes every agent better
off than η. For membership of NP, simply guess a normative system
η , and verify that for all i ∈ A, we have ui (K † η ) > ui (K † η)
- verifying requires a polynomial number of model checking 
problems, each of which takes polynomial time. Since η ⊆ R, the
normative system can be guessed in non-deterministic polynomial
time. For NP-hardness, we reduce IRNS, which we know to be 
NPcomplete from Theorem 2. Given an instance M of IRNS, we let M
in the instance of PARETO DOMINATED be as in the IRNS instance,
and define the normative system for PARETO DOMINATED to be η∅,
the empty normative system. Now, it is straightforward that there
exists a normative system η which Pareto dominates η∅ in M iff
there exist an individually rational normative system in M . Since
the complement problem is NP-complete, it follows that PENS is
co-NP-complete.
886 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
η0 η1 η2 η3 η4 η5 η6 η7 η8
u1(K † η) 4 4 7 6 5 0 0 8 0
u2(K † η) 4 7 4 6 0 5 8 0 0
Table 1: Utilities for all possible norms in our example
How about Pareto efficient norms for our toy example? Settling
this question amounts to finding the dominant normative systems
among η0 = η∅, η1, η2, η3 defined before, and η4 = {(s, t)}, η5 =
{(t, s)}, η6 = {(s, s), (t, s)}, η7 = {(t, t), (s, t)} and η8 =
{(s, t), (t, s)}. The utilities for each system are given in Table 1.
From this, we infer that the Pareto efficient norms are η1, η2, η3, η6
and η7. Note that η8 prohibits the resource to be passed from one
agent to another, and this is not good for any agent (since we have
chosen S0
= {s, t}, no agent can be sure to ever get the resource,
i.e., goal ϕi
1 is not true in K † η8).
5.3 Nash Implementation Normative Systems
The most famous solution concept in game theory is of course
Nash equilibrium [11, p.14]. A collection of strategies, one for each
agent, is said to form a Nash equilibrium if no agent can benefit by
doing anything other than playing its strategy, under the 
assumption that the other agents play theirs. Nash equilibria are important
because they provide stable solutions to the problem of what 
strategy an agent should play. Note that in our toy example, although
η3 is individually rational for each agent, it is not a Nash 
equilibrium, since given this norm, it would be beneficial for agent 1 to
deviate (and likewise for 2). In our framework, we say a social
system Σ = M , η (where η = η∅) is a Nash implementation if
SC (i.e., everyone complying with the normative system) forms a
Nash equilibrium in the game GΣ. The intuition is that if Σ is a
Nash implementation, then complying with the normative system
is a reasonable solution for all concerned: there can be no 
benefit to deviating from it, indeed, there is a positive incentive for all
to comply. If Σ is not a Nash implementation, then the normative
system is unlikely to succeed, since compliance is not rational for
some agents. (Our choice of terminology is deliberately chosen to
reflect the way the term Nash implementation is used in 
implementation theory, or mechanism design [11, p.185], where a game
designer seeks to achieve some outcomes by designing the rules of
the game such that these outcomes are equilibria.)
NASH IMPLEMENTATION (NI) :
Given: Multi-agent system M .
Question: Does there exist a non-empty normative 
system η over M such that M , η forms a Nash 
implementation?
Verifying that a particular social system forms a Nash 
implementation can be done in polynomial time - it amounts to checking:
∀i ∈ A : ui (K † η) ≥ ui (K † (η {i})).
This, clearly requires only a polynomial number of model checking
calls, each of which requires only polynomial time.
THEOREM 4. The NI problem is NP-complete, even for 
twoagent systems.
PROOF. For membership of NP, simply guess a normative 
system η and check that it forms a Nash implementation; since η ⊆ R,
guessing can be done in non-deterministic polynomial time, and as
s(2k+1)
1
1
1
1
1
1
11
1 1
11
2
2
2
2
2
2
2
2
2
2
2
t(x1)
f(x1)
t(x2)
f(x2)
t(xk)
f(xk)
2
2
t(x1)
f(x1)
t(x2)
f(x2)
t(xk)
f(xk)
......
s0
Figure 4: Reduction for Theorem 4.
we argued above, verifying that it forms a Nash implementation
can be done in polynomial time.
For NP-hardness, we reduce SAT. Suppose we are given a SAT 
instance ϕ over Boolean variables x1, . . . , xk . Then we construct an
instance of NI as follows. We create two agents, A = {1, 2}. For
each Boolean variable xi we create two Boolean variables, t(xi )
and f (xi ), and we then define a Kripke structure as shown in 
Figure 4, with s0 being the only initial state; the arc labelling in 
Figure 4 gives the α function, and each state is labelled with the 
propositions that are true in that state. For each Boolean variable xi , we
define the formulae xi and x⊥
i as follows:
xi = E f(t(xi ) ∧ E f((E f(t(xi ))) ∧ A f(¬f (xi ))))
x⊥
i = E f(f (xi ) ∧ E f((E f(f (xi ))) ∧ A f(¬t(xi ))))
Let ϕ∗
be the formula obtained from ϕ by systematically 
substituting xi for xi . Each agent has three goals: γi [0] = for both
i ∈ {1, 2}, while
γ1[1] =
k^
i=1
((E f(t(xi ))) ∧ (E f(f (xi ))))
γ2[1] = E fE f
k^
i=1
((E f(t(xi ))) ∧ (E f(f (xi ))))
and finally, for both agents, γi [2] being the conjunction of the 
following formulae:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 887
k^
i=1
(xi ∨ x⊥
i ) (3)
k^
i=1
¬(xi ∧ x⊥
i ) (4)
k^
i=1
¬(E f(t(xi )) ∧ E f(f (xi ))) (5)
ϕ∗
(6)
We denote the multi-agent system so constructed by Mϕ. Now,
we prove that the SAT instance ϕ is satisfiable iff Mϕ has a Nash
implementation normative system:
For the ⇒ direction, suppose ϕ is satisfiable, and let X be a
satisfying valuation, i.e., a set of Boolean variables making ϕ true.
We can extract from X a Nash implementation normative system η
as follows: if xi ∈ X , then η includes the arc from s0 to the state
in which f (xi ) is true, and also includes the arc from s(2k + 1)
to the state in which f (xi ) is true; if xi ∈ X , then η includes the
arc from s0 to the state in which t(xi ) is true, and also includes
the arc from s(2k + 1) to the state in which t(xi ) is true. No
other arcs, apart from those so defined, as included in η. Notice
that η is individually rational for both agents: if they both comply
with the normative system, then they will have their γi [2] goals
achieved, which they do not in the basic system. To see that η
forms a Nash implementation, observe that if either agent defects
from η, then neither will have their γi [2] goals achieved: agent 1
strictly prefers (C, C) over (D, C), and agent 2 strictly prefers
(C, C) over (C, D).
For the ⇐ direction, suppose there exists a Nash implementation
normative system η, in which case η = ∅. Then ϕ is satisfiable;
for suppose not. Then the goals γi [2] are not achievable by any
normative system, (by construction). Now, since η must forbid at
least one transition, then at least one agent would fail to have its
γi [1] goal achieved if it complied, so at least one would do better
by defecting, i.e., not complying with η. But this contradicts the
assumption that η is a Nash implementation, i.e., that (C, C) forms
a Nash equilibrium.
This result is perhaps of some technical interest beyond the specific
concerns of the present paper, since it is related to two problems
that are of wider interest: the complexity of mechanism design [5],
and the complexity of computing Nash equilibria [6, 7]
5.4 Richer Goal Languages
It is interesting to consider what happens to the complexity of
the problems we consider above if we allow richer languages for
goals: in particular, CTL
∗
[9]. The main difference is that 
determining ui (K) in a given multi-agent system M when such a goal 
language is used involves solving a PSPACE-complete problem (since
model checking for CTL
∗
is PSPACE-complete [8]). In fact, it seems
that for each of the three problems we consider above, the 
corresponding problem under the assumption of a CTL
∗
representation
for goals is also PSPACE-complete. It cannot be any easier, since 
determining the utility of a particular Kripke structure involves 
solving a PSPACE-complete problem. To see membership in PSPACE
we can exploit the fact that PSPACE = NPSPACE [12, p.150], and so
we can guess the desired normative system, applying a PSPACE
verification procedure to check that it has the desired properties.
6. CONCLUSIONS
Social norms are supposed to restrict our behaviour. Of course,
such a restriction does not have to be bad: the fact that an agent"s
behaviour is restricted may seem a limitation, but there may be 
benefits if he can assume that others will also constrain their behaviour.
The question then, for an agent is, how to be sure that others will
comply with a norm. And, for a system designer, how to be sure
that the system will behave socially, that is, according to its norm.
Game theory is a very natural tool to analyse and answer these
questions, which involve strategic considerations, and we have 
proposed a way to translate key questions concerning logic-based 
normative systems to game theoretical questions. We have proposed
a logical framework to reason about such scenarios, and we have
given some computational costs for settling some of the main 
questions about them. Of course, our approach is in many senses open
for extension or enrichment. An obvious issue is to consider is the
complexity of the questions we give for more practical 
representations of models (cf. [1]), and to consider other classes of allowable
goals.
7. REFERENCES
[1] T. Agotnes, W. van der Hoek, J. A. Rodriguez-Aguilar,
C. Sierra, and M. Wooldridge. On the logic of normative
systems. In Proc. IJCAI-07, Hyderabad, India, 2007.
[2] R. Alur, T. A. Henzinger, and O. Kupferman.
Alternating-time temporal logic. Jnl. of the ACM,
49(5):672-713, 2002.
[3] K. Binmore. Game Theory and the Social Contract Volume
1: Playing Fair. The MIT Press: Cambridge, MA, 1994.
[4] K. Binmore. Game Theory and the Social Contract Volume
2: Just Playing. The MIT Press: Cambridge, MA, 1998.
[5] V. Conitzer and T. Sandholm. Complexity of mechanism
design. In Proc. UAI, Edmonton, Canada, 2002.
[6] V. Conitzer and T. Sandholm. Complexity results about nash
equilibria. In Proc. IJCAI-03, pp. 765-771, Acapulco,
Mexico, 2003.
[7] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The
complexity of computing a Nash equilibrium. In Proc.
STOC, Seattle, WA, 2006.
[8] E. A. Emerson. Temporal and modal logic. In Handbook of
Theor. Comp. Sci. Vol. B, pages 996-1072. Elsevier, 1990.
[9] E. A. Emerson and J. Y. Halpern. ‘Sometimes" and ‘not
never" revisited: on branching time versus linear time
temporal logic. Jnl. of the ACM, 33(1):151-178, 1986.
[10] D. Fitoussi and M. Tennenholtz. Choosing social laws for
multi-agent systems: Minimality and simplicity. Artificial
Intelligence, 119(1-2):61-101, 2000.
[11] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
The MIT Press: Cambridge, MA, 1994.
[12] C. H. Papadimitriou. Computational Complexity.
Addison-Wesley: Reading, MA, 1994.
[13] Y. Shoham and M. Tennenholtz. On the synthesis of useful
social laws for artificial agent societies. In Proc. AAAI, San
Diego, CA, 1992.
[14] Y. Shoham and M. Tennenholtz. On social laws for artificial
agent societies: Off-line design. In Computational Theories
of Interaction and Agency, pages 597-618. The MIT Press:
Cambridge, MA, 1996.
[15] W. van der Hoek, M. Roberts, and M. Wooldridge. Social
laws in alternating time: Effectiveness, feasibility, and
synthesis. Synthese, 2007.
[16] M. Wooldridge and W. van der Hoek. On obligations and
normative ability. Jnl. of Appl. Logic, 3:396-420, 2005.
888 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
