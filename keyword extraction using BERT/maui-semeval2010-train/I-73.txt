Exchanging Reputation Values among Heterogeneous
Agent Reputation Models: An Experience on ART Testbed
Anarosa A. F. Brandão1
, Laurent Vercouter2
, Sara Casare1
and Jaime Sichman1
1
Laboratório de Técnicas Inteligentes - EP/USP
Av. Prof. Luciano Gualberto, 158, trav. 3, 05508-970, São Paulo - Brazil
+55 11 3091 5397
anarosabrandao@gmail.com, {sara.casare,jaime.sichman}@poli.usp.br
2
Ecole Nationale Supérieure des Mines de Saint-Etienne
158, cours Fauriel, 42023 Saint-Etienne Cedex 2, France
Laurent.Vercouter@emse.fr
ABSTRACT
In open MAS it is often a problem to achieve agents'
interoperability. The heterogeneity of its components turns the
establishment of interaction or cooperation among them into a
non trivial task, since agents may use different internal models
and the decision about trust other agents is a crucial condition to
the formation of agents' cooperation. In this paper we propose the
use of an ontology to deal with this issue. We experiment this idea
by enhancing the ART reputation model with semantic data
obtained from this ontology. This data is used during interaction
among heterogeneous agents when exchanging reputation values
and may be used for agents that use different reputation models.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent systems
General Terms
Design, Experimentation, Standardization.
1. INTRODUCTION
Open multiagent systems (MAS) are composed of autonomous
distributed agents that may enter and leave the agent society at
their will because open systems have no centralized control over
the development of its parts [1]. Since agents are considered as
autonomous entities, we cannot assume that there is a way to
control their internal behavior. These features are interesting to
obtain flexible and adaptive systems but they also create new risks
about the reliability and the robustness of the system. Solutions to
this problem have been proposed by the way of trust models
where agents are endowed with a model of other agents that
allows them to decide if they can or cannot trust another agent.
Such trust decision is very important because it is an essential
condition to the formation of agents' cooperation. The trust
decision processes use the concept of reputation as the basis of a
decision. Reputation is a subject that has been studied in several
works [4][5][8][9] with different approaches, but also with
different semantics attached to the reputation concept. Casare and
Sichman [2][3] proposed a Functional Ontology of Reputation
(FORe) and some directions about how it could be used to allow
the interoperability among different agent reputation models. This
paper describes how the FORe can be applied to allow
interoperability among agents that have different reputation
models. An outline of this approach is sketched in the context of a
testbed for the experimentation and comparison of trust models,
the ART testbed [6].
2. THE FUNCTIONAL ONTOLOGY OF
REPUTATION (FORe)
In the last years several computational models of reputation have
been proposed [7][10][13][14]. As an example of research
produced in the MAS field we refer to three of them: a cognitive
reputation model [5], a typology of reputation [7] and the
reputation model used in the ReGret system [9][10]. Each model
includes its own specific concepts that may not exist in other
models, or exist with a different name. For instance, Image and
Reputation are two central concepts in the cognitive reputation
model. These concepts do not exist in the typology of reputation
or in the ReGret model. In the typology of reputation, we can find
some similar concepts such as direct reputation and indirect
reputation but there are some slight semantic differences. In the
same way, the ReGret model includes four kinds of reputation
(direct, witness, neighborhood and system) that overlap with the
concepts of other models but that are not exactly the same.
The Functional Ontology of Reputation (FORe) was defined as a
common semantic basis that subsumes the concepts of the main
reputation models. The FORe includes, as its kernel, the following
concepts: reputation nature, roles involved in reputation formation
and propagation, information sources for reputation, evaluation of
reputation, and reputation maintenance. The ontology concept
ReputationNature is composed of concepts such as
IndividualReputation, GroupReputation and ProductReputation.
Reputation formation and propagation involves several roles,
played by the entities or agents that participate in those processes.
The ontology defines the concepts ReputationProcess and
ReputationRole. Moreover, reputation can be classified according
to the origin of beliefs and opinions that can derive from several
sources. The ontology defines the concept ReputationType which
can be PrimaryReputation or SecondaryReputation.
PrimaryReputation is composed of concepts ObservedReputation
and DirectReputation and the concept SecondaryReputation is
composed of concepts such as PropagatedReputation and
CollectiveReputation. More details about the FORe can be found
on [2][3].
3. MAPPING THE AGENT REPUTATION
MODELS TO THE FORe
Visser et al [12] suggest three different ways to support semantic
integration of different sources of information: a centralized
approach, where each source of information is related to one
common domain ontology; a decentralized approach, where every
source of information is related to its own ontology; and a hybrid
approach, where every source of information has its own ontology
and the vocabulary of these ontologies are related to a common
ontology. This latter organizes the common global vocabulary in
order to support the source ontologies comparison. Casare and
Sichman [3] used the hybrid approach to show that the FORe
serves as a common ontology for several reputation models.
Therefore, considering the ontologies which describe the agent
reputation models we can define a mapping between these
ontologies and the FORe whenever the ontologies use a common
vocabulary. Also, the information concerning the mappings
between the agent reputation models and the FORe can be directly
inferred by simply classifying the resulting ontology from the
integration of a given reputation model ontology and the FORe in
an ontology tool with reasoning engine.
For instance, a mapping between the Cognitive Reputation Model
ontology and the FORe relates the concepts Image and Reputation
to PrimaryReputation and SecondaryReputation from FORe,
respectively. Also, a mapping between the Typology of
Reputation and the FORe relates the concepts Direct Reputation
and Indirect Reputation to PrimaryReputation and
SecondaryReputation from FORe, respectively. Nevertheless, the
concepts Direct Trust and Witness Reputation from the Regret
System Reputation Model are mapped to PrimaryReputation and
PropagatedReputation from FORe. Since PropagatedReputation is
a sub-concept of SecondaryReputation, it can be inferred that
Witness Reputation is also mapped to SecondaryReputation.
4. EXPERIMENTAL SCENARIOS USING
THE ART TESTBED
To exemplify the use of mappings from last section, we define a
scenario where several agents are implemented using different
agent reputation models. This scenario includes the agents"
interaction during the simulation of the game defined by ART [6]
in order to describe the ways interoperability is possible between
different trust models using the FORe.
4.1 The ART testbed
The ART testbed provides a simulation engine on which several
agents, using different trust models, may run. The simulation
consists in a game where the agents have to decide to trust or not
other agents. The game"s domain is art appraisal, in which agents
are required to evaluate the value of paintings based on
information exchanged among other agents during agents"
interaction. The information can be an opinion transaction, when
an agent asks other agents to help it in its evaluation of a painting;
or a reputation transaction, when the information required is
about the reputation of another agent (a target) for a given era.
More details about the ART testbed can be found in [6].
The ART common reputation model was enhanced with semantic
data obtained from FORe. A general agent architecture for
interoperability was defined [11] to allow agents to reason about
the information received from reputation interactions. This
architecture contains two main modules: the Reputation Mapping
Module (RMM) which is responsible for mapping concepts
between an agent reputation model and FORe; and the Reputation
Reasoning Module (RRM) which is responsible for deal with
information about reputation according to the agent reputation
model.
4.2 Reputation transaction scenarios
While including the FORe to the ART common reputation model,
we have incremented it to allow richer interactions that involve
reputation transaction. In this section we describe scenarios
concerning reputation transactions in the context of ART testbed,
but the first is valid for any kind of reputation transaction and the
second is specific for the ART domain.
4.2.1 General scenario
Suppose that agents A, B and C are implemented according to the
aforementioned general agent architecture with the enhanced ART
common reputation model, using different reputation models.
Agent A uses the Typology of Reputation model, agent B uses the
Cognitive Reputation Model and agent C uses the ReGret System
model. Consider the interaction about reputation where agents A
and B receive from agent C information about the reputation of
agent Y. A big picture of this interaction is showed in Figure 2.
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
C
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
A
CogMod.
Ontology
(Y, value=0.8,
reputation)
B
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
C
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
(Y, value=0.8,
witnessreputation)
C
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
A
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
(Y, value=0.8,
propagatedreputation)
A
CogMod.
Ontology
(Y, value=0.8,
reputation)
B
CogMod.
Ontology
(Y, value=0.8,
reputation)
CogMod.
Ontology
(Y, value=0.8,
reputation)
(Y, value=0.8,
reputation)
B
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
Figure 1. Interaction about reputation
The information witness reputation from agent C is treated by its
RMM and is sent as PropagatedReputation to both agents. The
corresponding information in agent A reputation model is
propagated reputation and in agent B reputation model is
reputation. The way agents A and B make use of the information
depends on their internal reputation model and their RRM
implementation.
1048 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
4.2.2 ART scenario
Considering the same agents A and B and the art appraisal domain
of ART, another interesting scenario describes the following
situation: agent A asks to agent B information about agents it
knows that have skill on some specific painting era. In this case
agent A wants information concerning the direct reputation agent
B has about agents that have skill on an specific era, such as
cubism. Following the same steps of the previous scenario, agent
A message is prepared in its RRM using information from its
internal model. A big picture of this interaction is in Figure 2.
Typol.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = directreputation)
A
(agent = ?, value = ?, skill = cubism,
reputation = PrimaryReputation)
CogMod.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = image)
B
Typol.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = directreputation)
A
(agent = ?, value = ?, skill = cubism,
reputation = PrimaryReputation)
CogMod.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = image)
B
Figure 2. Interaction about specific types of reputation values
Agent B response to agent A is processed in its RRM and it is
composed of tuples (agent, value, cubism, image) , where
the pair (agent, value) is composed of all agents and associated
reputation values whose agent B knows their expertise about
cubism by its own opinion. This response is forwarded to the
RMM in order to be translated to the enriched common model and
to be sent to agent A. After receiving the information sent by
agent B, agent A processes it in its RMM and translates it to its
own reputation model to be analyzed by its RRM.
5. CONCLUSION
In this paper we present a proposal for reducing the
incompatibility between reputation models by using a general
agent architecture for reputation interaction which relies on a
functional ontology of reputation (FORe), used as a globally
shared reputation model. A reputation mapping module allows
agents to translate information from their internal reputation
model into the shared model and vice versa. The ART testbed has
been enriched to use the ontology during agent transactions. Some
scenarios were described to illustrate our proposal and they seem
to be a promising way to improve the process of building
reputation just using existing technologies.
6. ACKNOWLEDGMENTS
Anarosa A. F. Brandão is supported by CNPq/Brazil grant
310087/2006-6 and Jaime Sichman is partially supported by
CNPq/Brazil grants 304605/2004-2, 482019/2004-2 and
506881/2004-1. Laurent Vercouter was partially supported by
FAPESP grant 2005/02902-5.
7. REFERENCES
[1] Agha, G. A. Abstracting Interaction Patterns: A
Programming Paradigm for Open Distributed Systems, In
(Eds) E. Najm and J.-B. Stefani, Formal Methods for Open
Object-based Distributed Systems IFIP Transactions, 1997,
Chapman Hall.
[2] Casare,S. and Sichman, J.S. Towards a Functional Ontology
of Reputation, In Proc of the 4th
Intl Joint Conference on
Autonomous Agents and Multi Agent Systems (AAMAS"05),
Utrecht, The Netherlands, 2005, v.2, pp. 505-511.
[3] Casare, S. and Sichman, J.S. Using a Functional Ontology of
Reputation to Interoperate Different Agent Reputation
Models, Journal of the Brazilian Computer Society, (2005),
11(2), pp. 79-94.
[4] Castelfranchi, C. and Falcone, R. Principles of trust in MAS:
cognitive anatomy, social importance and quantification. In
Proceedings of ICMAS"98, Paris, 1998, pp. 72-79.
[5] Conte, R. and Paolucci, M. Reputation in Artificial Societies:
Social Beliefs for Social Order, Kluwer Publ., 2002.
[6] Fullam, K.; Klos, T.; Muller, G.; Sabater, J.; Topol, Z.;
Barber, S.;Rosenchein, J.; Vercouter, L. and Voss, M. A
specification of the agent reputation and trust (art) testbed:
experimentation and competition for trust in agent societies.
In Proc. of the 4th
Intl. Joint Conf on Autonomous Agents
and Multiagent Systems (AAMAS"05), ACM, 2005, 512-158.
[7] Mui, L.; Halberstadt, A.; Mohtashemi, M. Notions of
Reputation in Multi-Agents Systems: A Review. In: Proc of
1st Intl. Joint Conf. on Autonomous Agents and Multi-agent
Systems (AAMAS 2002), Bologna, Italy, 2002, 1, 280-287.
[8] Muller, G. and Vercouter, L. Decentralized monitoring of
agent communication with a reputation model. In Trusting
Agents for Trusting Electronic Societies, LNCS 3577, 2005,
pp. 144-161.
[9] Sabater, J. and Sierra, C. ReGret: Reputation in gregarious
societies. In Müller, J. et al (Eds) Proc. of the 5th
Intl. Conf.
on Autonomous Agents, Canada, 2001, ACM, 194-195.
[10] Sabater, J. and Sierra, C. Review on Computational Trust
and Reputation Models. In: Artificial Intelligence Review,
Kluwer Acad. Publ., (2005), v. 24, n. 1, pp. 33 - 60.
[11] Vercouter,L, Casare, S., Sichman, J. and Brandão, A. An
experience on reputation models interoperability based on a
functional ontology In Proc. of the 20th
IJCAI, Hyderabad,
India, 2007, pp.617-622.
[12] Visser, U.; Stuckenschmidt, H.; Wache, H. and Vogele, T.
Enabling technologies for inter-operability. In: In U. Visser
and H. Pundt, Eds, Workshop on the 14th Intl Symp. of
Computer Science for Environmental Protection, Bonn,
Germany, 2000, pp. 35-46.
[13] Yu, B. and Singh, M.P. An Evidential Model of Distributed
Reputation Management. In: Proc. of the 1st Intl Joint Conf.
on Autonomous Agents and Multi-agent Systems (AAMAS
2002), Bologna, Italy, 2002, part 1, pp. 294 - 301.
[14] Zacharia, G. and Maes, P. Trust Management Through
Reputation Mechanisms. In: Applied Artificial Intelligence,
14(9), 2000, pp. 881-907.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1049
